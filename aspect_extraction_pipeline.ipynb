{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_analysis(df): \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment,pos,lemma', use_gpu=True, verbose=False)\n",
    "    rows_list = []\n",
    "    for row in df.itertuples():\n",
    "        doc = nlp(str(row.reviewText))\n",
    "        for sentence in doc.sentences:\n",
    "            sentiment = sentno_to_word(int(sentence.sentiment))\n",
    "            for word in sentence.words:\n",
    "                dict_ = {}\n",
    "                dict_.update({'item_id': str(row.itemID)})\n",
    "                dict_.update({'review_id': str(row.itemID) + str(row.reviewerID)}) \n",
    "                dict_.update({'word': str(word.lemma)}) \n",
    "                dict_.update({'sentiment': sentiment})\n",
    "                dict_.update({'upos': str(word.upos)})\n",
    "                dict_.update({'xpos': str(word.xpos)})\n",
    "                rows_list.append(dict_)\n",
    "    sentiment_df = pd.DataFrame(rows_list)\n",
    "    return sentiment_df\n",
    "\n",
    "def sentno_to_word(number):\n",
    "    switcher = {\n",
    "        0: 'negative',\n",
    "        1: 'neutral',\n",
    "        2: 'positive'\n",
    "    }\n",
    "    sentiment_string = (switcher.get(number))\n",
    "    return sentiment_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(word, stopword_list):\n",
    "    if word in stopword_list:\n",
    "        return True\n",
    "    elif re.fullmatch(r'[a-zA-Z0-9]{1}[a-zA-Z\\- ]*[a-zA-Z0-9]{1}', word) == None:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unigrams(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    #words coming from negative and neutral sentiment sentences removal\n",
    "    df.drop(df[ ~( df['sentiment'] == 'positive' ) ].index, inplace=True)\n",
    "\n",
    "    #filtering based on xpos\n",
    "    df.drop(df[ ~( (df['xpos'] == 'JJ') | (df['upos'] == 'NOUN') ) ].index, inplace=True) \n",
    "\n",
    "    #stopwords removal\n",
    "    with open(Path(\"stopwords/unigrams_stopwords.txt\"), 'r') as file:\n",
    "        stopword_list = [line.rstrip('\\n') for line in file]\n",
    "    df['stop'] = df.word.map(lambda x: is_stopword(x, stopword_list))\n",
    "    df.drop(df[(df['stop'] == True)].index, inplace=True)\n",
    "    del df['stop']\n",
    "    \n",
    "    # Join unigrams into a list and return\n",
    "    df = df.groupby('item_id', as_index=False, sort=False)[['word']].agg(lambda x: ','.join(x)) # group unigrams by item\n",
    "    df.columns = ['item_id', 'unigrams']\n",
    "    df['unigrams'] = df['unigrams'].str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(df):\n",
    "    df = df.copy()\n",
    "    # Filter bigrams using pos-tags\n",
    "    df = pd.concat([df, df.shift(-1).add_prefix('next_')], axis=1) #create bigrams\n",
    "    df.drop(df[df['review_id'] != df['next_review_id']].index, inplace=True) #delete rows with bigrams from different reviews\n",
    "    df.drop(df[ (df['sentiment'] != 'positive') | (df['next_sentiment'] != 'positive') ].index, inplace=True)\n",
    "\n",
    "    #filtering based on xpos\n",
    "    df.drop(df[~( ( (df['xpos']  == 'JJ') & (df['next_upos']  == 'NOUN') ) | \\\n",
    "                  ( (df['upos']  == 'NN') & (df['next_upos']  == 'NOUN') ) )].index, inplace=True)\n",
    "    \n",
    "    #unigram stopword removal\n",
    "    with open(Path(\"stopwords/bigrams_single_stopwords.txt\"), 'r') as file:\n",
    "      stopword_list = [line.rstrip('\\n') for line in file]\n",
    "    for stopword in stopword_list:\n",
    "      df.drop(df[((df['word'] == stopword) | (df['next_word'] == stopword))].index, inplace=True)\n",
    "    \n",
    "    #Create bigrams\n",
    "    df['bigrams'] = df['word'] + ' ' + df['next_word']\n",
    "\n",
    "    #Remove bigrams stopwords\n",
    "    with open(Path(\"stopwords/bigrams_stopwords.txt\"), 'r') as file:\n",
    "        stopword_list = [line.rstrip('\\n') for line in file]\n",
    "    df['stop'] = df['bigrams'].map(lambda x: is_stopword(x, stopword_list))\n",
    "    df.drop(df[(df['stop'] == True)].index, inplace=True)\n",
    "    del df['stop']\n",
    "\n",
    "    #Aggregate bigrams by item\n",
    "    df = df.groupby('item_id', as_index=False, sort=False)[['bigrams']].agg(lambda x: ','.join(x))\n",
    "    df['bigrams'] = df['bigrams'].str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(df, mode):\n",
    "    '''\n",
    "    Returns a Dataframe containing the term frequency for each token/bigram aggregated by item.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing unigrams or bigrams, it must be the output of extract_unigrams or extract_bigrams.\n",
    "        mode: String that specifies if working with unigrams or bigrams, it must be 'unigrams' or 'bigrams'.\n",
    "\n",
    "    Returns:\n",
    "        tf_concat: The Dataframe with the term frequency for each token/bigram.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If mode not specified.\n",
    "    '''\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if not mode:\n",
    "         raise Exception(\"You must set a mode, available modes are 'unigrams' and 'bigrams'\") \n",
    "\n",
    "    tf_concat = pd.DataFrame()\n",
    "    for row in df.itertuples():\n",
    "        if mode == 'unigrams':\n",
    "            wordlist = row.unigrams.split(',')\n",
    "        else:\n",
    "            wordlist = row.bigrams.split(',')\n",
    "        dictionary = wordListToFreqDict(wordlist)\n",
    "        sorted_dict = sortFreqDict(dictionary)\n",
    "        tf_df = pd.DataFrame(sorted_dict, columns=['tf', 'term'])\n",
    "        tf_df['item_id'] = str(row.item_id)\n",
    "        tf_concat = pd.concat([tf_concat, tf_df])\n",
    "    tf_concat.sort_values(by='item_id', inplace=True, ascending=False)\n",
    "    tf_concat.reset_index(drop=True, inplace=True)\n",
    "    return tf_concat\n",
    "\n",
    "def wordListToFreqDict(wordlist):\n",
    "    '''\n",
    "    Returns a frequency dictionary for the wordlist.\n",
    "\n",
    "    Args:\n",
    "        wordlist: The list of terms.\n",
    "\n",
    "    Returns:\n",
    "        dict1: The frequency dictionary.\n",
    "    '''\n",
    "    \n",
    "    doc_dim = len(wordlist)\n",
    "    wordfreq = [wordlist.count(p)/doc_dim for p in wordlist]\n",
    "    dict_ = dict(list(zip(wordlist,wordfreq)))\n",
    "    return dict_\n",
    "\n",
    "def sortFreqDict(freqdict):\n",
    "    '''\n",
    "    Returns the sorted frequency dictionary.\n",
    "\n",
    "    Args:\n",
    "        freqdict: The frequence dictionary to sort.\n",
    "\n",
    "    Returns:\n",
    "        aux: The sorted frequency dictionary.\n",
    "    '''\n",
    "    \n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_tfidf(item_ids, unigrams, bigrams, unigrams_tf, bigrams_tf):\n",
    "  '''\n",
    "  Calculates the tf-idf.\n",
    "\n",
    "  Args:\n",
    "      item_ids: list of item ids to consider during tf-idf calculation\n",
    "      unigrams: dataframe with all the extracted unigrams, it can be obtained by joining together all partial dataframes\n",
    "      bigrams: dataframe with all the extracted bigrams, it can be obtained by joining together all partial dataframes\n",
    "      unigrams_tf: dataframe with the unigrams term frequency, it can be obtained by joining together all partial dataframes\n",
    "      bigrams_tf: dataframe with the bigrams term frequency, it can be obtained by joining together all partial dataframes\n",
    "\n",
    "  '''\n",
    "  items_number = len(item_ids)\n",
    "\n",
    "  all_items_path = 'all_items'\n",
    "  if not os.path.exists(all_items_path):\n",
    "    os.makedirs(all_items_path)\n",
    "\n",
    "  # unigrams document frequency\n",
    "  unigrams_df = df(unigrams, 'unigrams')\n",
    "  path = os.path.join(all_items_path, 'unigrams_df.txt')\n",
    "  unigrams_df.to_csv(path, sep=\"|\", header=['term', 'df'], index=None)\n",
    "\n",
    "  # unigrams tf-idf for each item\n",
    "  unigrams_tfidf = tf_idf(unigrams_tf, unigrams_df, items_number) # calculate unigrams tfidf\n",
    "  path = os.path.join(all_items_path, 'unigrams_tfidf.txt')\n",
    "  unigrams_tfidf.to_csv(path, sep=\"|\", header=['term', 'item_id', 'tf_idf'], index=None)\n",
    "\n",
    "  # Bigrams document frequency\n",
    "  bigrams_df = df(bigrams, 'bigrams')\n",
    "  path = os.path.join(all_items_path, 'bigrams_df.txt')\n",
    "  bigrams_df.to_csv(path, sep=\"|\", header=['term', 'df'], index=None)\n",
    "    \n",
    "  # Bigrams tf-idf for each item\n",
    "  bigrams_tfidf = tf_idf(bigrams_tf, bigrams_df, items_number)\n",
    "  path = os.path.join(all_items_path, 'bigrams_tfidf.txt')\n",
    "  bigrams_tfidf.to_csv(path, sep=\"|\", header=['term', 'item_id', 'tf_idf'], index=None)\n",
    "  \n",
    "  return\n",
    "\n",
    "def df(df, mode):\n",
    "    '''\n",
    "    Returns a Dataframe containing the document frequency for each token/bigram.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing unigrams or bigrams, it must be the output of extract_unigrams or extract_bigrams.\n",
    "        mode: String that specifies if working with unigrams or bigrams, it must be 'unigrams' or 'bigrams'.\n",
    "\n",
    "    Returns:\n",
    "        item_df: The Dataframe with the document frequency for each token/bigram.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If mode not specified.\n",
    "    '''\n",
    "    \n",
    "    if not mode:\n",
    "         raise Exception(\"You must set a mode, available modes are 'unigrams' and 'bigrams'\") \n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    df_dict = {}\n",
    "    for row in df.itertuples():\n",
    "        if mode == 'unigrams':\n",
    "            items = row.unigrams.split(',')\n",
    "        else:\n",
    "            items = row.bigrams.split(',')\n",
    "        for w in items:\n",
    "            try:\n",
    "                df_dict[w].add(row.item_id)\n",
    "            except:\n",
    "                df_dict[w] = {row.item_id}\n",
    "    for i in df_dict:\n",
    "        df_dict[i] = len(df_dict[i])\n",
    "    item_df = pd.DataFrame.from_dict(df_dict, orient='index').reset_index()\n",
    "    item_df.columns = np.arange(len(item_df.columns))\n",
    "    item_df.columns = ['term', 'df']\n",
    "    item_df.sort_values(by='df', inplace=True, ascending=False)\n",
    "    item_df = item_df.reset_index()\n",
    "    del item_df['index']\n",
    "    return item_df\n",
    "\n",
    "def tf_idf(tf, df, doc_number):\n",
    "    '''\n",
    "    Returns a Dataframe containing the tf-idf for each token/bigram.\n",
    "\n",
    "    Args:\n",
    "        tf: The DataFrame containing term frequency, it must be the output of tf.\n",
    "        df: The DataFrame containing document frequency, it must be the output of df.\n",
    "        doc_number: The number of documents for idf calculation, it is equal to the number of items analyzed\n",
    "\n",
    "    Returns:\n",
    "        tf_idf_df: The Dataframe with the tf-idf score for each token/bigram.\n",
    "    '''\n",
    "    \n",
    "    tf = tf.copy()\n",
    "    tf['tf'] = tf['tf'].astype(float)\n",
    "    tf['term'] = tf['term'].astype(str)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['df'] = df['df'].astype(int)\n",
    "    df['term'] = df['term'].astype(str)\n",
    "    \n",
    "    tf_idf_df = tf.merge(df, on=\"term\")\n",
    "    tf_idf_df['tf_idf'] = tf_idf_df['tf'] * np.log10(doc_number/tf_idf_df['df'])\n",
    "    del tf_idf_df['tf']\n",
    "    del tf_idf_df['df']\n",
    "    tf_idf_df.sort_values(by='tf_idf', inplace=True, ascending=False)\n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborazione dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amazon_items_reviews = pd.read_csv('dataset.csv')\n",
    "#annotated_df = lexical_analysis(amazon_items_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrazione aspetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the annotated dataset\n",
    "annotated_df = pd.read_csv('annotated_dataset_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = annotated_df['item_id'].unique()\n",
    "\n",
    "all_items_path = 'all_items'\n",
    "if not os.path.exists(all_items_path):\n",
    "    os.makedirs(all_items_path)\n",
    "    \n",
    "unigrams = 'unigrams'\n",
    "if not os.path.exists('unigrams'):\n",
    "    os.makedirs('unigrams')\n",
    "\n",
    "bigrams = 'bigrams'\n",
    "if not os.path.exists('bigrams'):\n",
    "    os.makedirs('bigrams')\n",
    "\n",
    "# Unigrams extraction\n",
    "unigrams = extract_unigrams(annotated_df)\n",
    "unigrams.to_csv(os.path.join(all_items_path, 'unigrams' + '.txt'), sep=\"|\", header=['item_id', 'unigrams'], index=False)\n",
    "unigrams_tf = tf(unigrams, 'unigrams')\n",
    "unigrams_tf.to_csv(os.path.join(all_items_path, 'unigrams_tf' + '.txt'), sep=\"|\", header=['tf', 'term', 'item_id'], index=False)\n",
    "\n",
    "# Bigrams extraction\n",
    "bigrams = extract_bigrams(annotated_df)\n",
    "bigrams.to_csv(os.path.join(all_items_path, 'bigrams' + '.txt'), sep=\"|\", header=['item_id', 'bigrams'], index=False)\n",
    "bigrams_tf = tf(bigrams, 'bigrams')\n",
    "bigrams_tf.to_csv(os.path.join(all_items_path, 'bigrams_tf' + '.txt'), sep=\"|\", header=['tf', 'term', 'item_id'], index=False)\n",
    "\n",
    "# tf-idf\n",
    "total_tfidf(item_ids, unigrams, bigrams, unigrams_tf, bigrams_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q7305024</td>\n",
       "      <td>combination,story,era,vivid,rich,bitter,comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q5460091</td>\n",
       "      <td>course,attractive,hostage,man,admiration,help,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q2609885</td>\n",
       "      <td>young,thief,endearing,engaging,level,nice,spel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1764445</td>\n",
       "      <td>previous,series,informative,interesting,story,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q731626</td>\n",
       "      <td>thougth,provoking,phenominal,tact,wit,heart,mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>Q7970705</td>\n",
       "      <td>gift,friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>Q3882176</td>\n",
       "      <td>glad,collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>Q29384930</td>\n",
       "      <td>excellent,overview,major,change,sound,picture,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>Q7225574</td>\n",
       "      <td>edition,unforgettable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>Q7619050</td>\n",
       "      <td>pseudo-victorian,language,mix,fantastic,villai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1489 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                                           unigrams\n",
       "0      Q7305024  combination,story,era,vivid,rich,bitter,comple...\n",
       "1      Q5460091  course,attractive,hostage,man,admiration,help,...\n",
       "2      Q2609885  young,thief,endearing,engaging,level,nice,spel...\n",
       "3      Q1764445  previous,series,informative,interesting,story,...\n",
       "4       Q731626  thougth,provoking,phenominal,tact,wit,heart,mi...\n",
       "...         ...                                                ...\n",
       "1484   Q7970705                                        gift,friend\n",
       "1485   Q3882176                                    glad,collection\n",
       "1486  Q29384930  excellent,overview,major,change,sound,picture,...\n",
       "1487   Q7225574                              edition,unforgettable\n",
       "1488   Q7619050  pseudo-victorian,language,mix,fantastic,villai...\n",
       "\n",
       "[1489 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of aspects file for each book\n",
    "unigrams_tfidf = pd.read_csv(os.path.join(all_items_path, 'unigrams_tfidf.txt'), sep='|')\n",
    "bigrams_tfidf = pd.read_csv(os.path.join(all_items_path, 'bigrams_tfidf.txt'), sep='|')\n",
    "\n",
    "for id in item_ids:\n",
    "    item_tfidf = unigrams_tfidf[unigrams_tfidf['item_id'] == str(id)]\n",
    "    item_tfidf.to_csv(os.path.join('unigrams', str(id) + '.txt'), sep='|', columns=['term', 'tf_idf'], index=None)\n",
    "\n",
    "    item_tfidf = bigrams_tfidf[bigrams_tfidf['item_id'] == str(id)]\n",
    "    item_tfidf.to_csv(os.path.join('bigrams', str(id) + '.txt'), sep='|', columns=['term', 'tf_idf'], index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selezione aspetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and save unigrams topK\n",
    "unigrams_path = 'unigrams/'\n",
    "unigrams_topK_path = 'unigrams_top'+ str(K) + '/'\n",
    "if not os.path.exists(unigrams_topK_path):\n",
    "    os.makedirs(unigrams_topK_path)\n",
    "unigrams_list = os.listdir(unigrams_path)\n",
    "\n",
    "for book_aspects in unigrams_list:\n",
    "    df = pd.read_csv(unigrams_path+book_aspects, sep='|')\n",
    "    top50_df = df.head(K)\n",
    "    top50_df.to_csv( (unigrams_topK_path + book_aspects), sep='|', index = None)\n",
    "\n",
    "\n",
    "# Select and save bigrams topK\n",
    "bigrams_path = 'bigrams/'\n",
    "bigrams_topK_path = 'bigrams_top'+ str(K) + '/'\n",
    "if not os.path.exists(bigrams_topK_path):\n",
    "    os.makedirs(bigrams_topK_path)\n",
    "bigrams_list = os.listdir(bigrams_path)\n",
    "\n",
    "for book_aspects in bigrams_list:\n",
    "    df = pd.read_csv(bigrams_path+book_aspects, sep='|')\n",
    "    top50_df = df.head(K)\n",
    "    top50_df.to_csv( (bigrams_topK_path + book_aspects), sep='|', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifica bigrammi positivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_path = 'bigrams_top'+ str(K) + '/'\n",
    "bigrams_list = os.listdir(bigrams_path)\n",
    "rows_list = []\n",
    "\n",
    "with open('stopwords/positive_adjectives.txt', 'r') as file:\n",
    "    temp = file.read().splitlines()\n",
    "    pos_adjs = [adj for adj in temp]\n",
    "\n",
    "for book_aspects in bigrams_list:\n",
    "    df = pd.read_csv(bigrams_path+book_aspects, sep='|')\n",
    "    for row in df.itertuples():\n",
    "        for adj in pos_adjs:\n",
    "            if row.term.startswith(adj):\n",
    "                unigram = row.term.replace(adj+' ','')             \n",
    "                df.drop(df[df['term'] == row.term].index, inplace=True)\n",
    "                unigram_row = {}\n",
    "                unigram_row.update({'item_id': book_aspects.replace('.txt', '')})\n",
    "                unigram_row.update({'term': unigram})\n",
    "                unigram_row.update({'tf_idf': row.tf_idf})\n",
    "                rows_list.append(unigram_row)\n",
    "    df.to_csv((bigrams_path + book_aspects), sep='|', index = None)\n",
    "\n",
    "additional_unigrams = pd.DataFrame(rows_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords removal\n",
    "with open(Path(\"stopwords/unigrams_stopwords.txt\"), 'r') as file:\n",
    "    stopword_list = [line.rstrip('\\n') for line in file]\n",
    "additional_unigrams['stop'] = additional_unigrams.term.map(lambda x: is_stopword(x, stopword_list))\n",
    "additional_unigrams.drop(additional_unigrams[(additional_unigrams['stop'] == True)].index, inplace=True)\n",
    "del additional_unigrams['stop']\n",
    "\n",
    "unigrams_path = 'unigrams_top'+ str(K) + '/'\n",
    "\n",
    "for row in additional_unigrams.itertuples():\n",
    "    item_id = row.item_id\n",
    "    unigram = row.term\n",
    "    item_unigrams = pd.read_csv(unigrams_path + item_id + '.txt', sep=\"|\")\n",
    "    u = item_unigrams['term'].tolist()\n",
    "    if unigram not in u:\n",
    "        with open(unigrams_path + item_id + '.txt', 'a') as file:\n",
    "            file.write(f'{unigram}|{row.tf_idf}\\r')\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>term</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q10263869</td>\n",
       "      <td>trance</td>\n",
       "      <td>0.021193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q10263869</td>\n",
       "      <td>snapshot</td>\n",
       "      <td>0.021193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1027487</td>\n",
       "      <td>example</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1027487</td>\n",
       "      <td>layla</td>\n",
       "      <td>0.009405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1027487</td>\n",
       "      <td>beautifull imagination</td>\n",
       "      <td>0.009405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>Q964269</td>\n",
       "      <td>anti-hero</td>\n",
       "      <td>0.027643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>Q978135</td>\n",
       "      <td>kerouac</td>\n",
       "      <td>0.127159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>Q978135</td>\n",
       "      <td>creator</td>\n",
       "      <td>0.115118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>Q978135</td>\n",
       "      <td>musician</td>\n",
       "      <td>0.115118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>Q978135</td>\n",
       "      <td>rhythm</td>\n",
       "      <td>0.103077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5377 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                    term    tf_idf\n",
       "0     Q10263869                  trance  0.021193\n",
       "1     Q10263869                snapshot  0.021193\n",
       "4      Q1027487                 example  0.010438\n",
       "5      Q1027487                   layla  0.009405\n",
       "6      Q1027487  beautifull imagination  0.009405\n",
       "...         ...                     ...       ...\n",
       "7009    Q964269               anti-hero  0.027643\n",
       "7010    Q978135                 kerouac  0.127159\n",
       "7011    Q978135                 creator  0.115118\n",
       "7012    Q978135                musician  0.115118\n",
       "7013    Q978135                  rhythm  0.103077\n",
       "\n",
       "[5377 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione file popolazione KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pd.read_csv('annotated_dataset_big.csv')\n",
    "item_ids = annotated_df['item_id'].unique()\n",
    "u_topK_path = 'unigrams_top'+ str(K) + '/'\n",
    "b_topK_path = 'bigrams_top'+ str(K) + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_unigrams = []\n",
    "no_unigrams_item_ids = []\n",
    "\n",
    "for id in item_ids:\n",
    "    try:\n",
    "        with open(os.path.join(u_topK_path, id + '.txt'), 'r') as file:\n",
    "            item_unigrams = [re.sub(r'[|]{1}.*?[\\n]{1}','', aspect) for aspect in file.readlines()[1:]]\n",
    "            all_items_unigrams.extend([id, unigram] for unigram in item_unigrams)\n",
    "    except FileNotFoundError:\n",
    "        no_unigrams_item_ids.append(id)\n",
    "\n",
    "unigrams_df = pd.DataFrame(all_items_unigrams, columns=['item_id', 'aspect'])\n",
    "unigrams_ids = unigrams_df.drop(labels='item_id', axis=1)\n",
    "unigrams_ids.drop_duplicates(inplace=True)\n",
    "unigrams_ids.reset_index(drop=True, inplace=True)\n",
    "unigrams_ids.insert(0, 'id', [id for id in range(1, len(unigrams_ids) + 1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_bigrams = []\n",
    "no_bigrams_item_ids = []\n",
    "\n",
    "for id in item_ids:\n",
    "    try:\n",
    "        with open(os.path.join(b_topK_path, id + '.txt'), 'r') as file:\n",
    "            item_bigrams = [re.sub(r'[|]{1}.*?[\\n]{1}','', aspect) for aspect in file.readlines()[1:]]\n",
    "            all_items_bigrams.extend([id, bigram] for bigram in item_bigrams)\n",
    "    except FileNotFoundError:\n",
    "        no_bigrams_item_ids.append(id)\n",
    "\n",
    "bigrams_df = pd.DataFrame(all_items_bigrams, columns=['item_id', 'aspect'])\n",
    "bigrams_ids = bigrams_df.drop(labels='item_id', axis=1)\n",
    "bigrams_ids.drop_duplicates(inplace=True)\n",
    "bigrams_ids.reset_index(drop=True, inplace=True)\n",
    "bigrams_ids.insert(0, 'id', [id for id in range(1_000_000, len(bigrams_ids) + 1_000_000, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects_df = pd.concat([unigrams_df, bigrams_df])\n",
    "aspects_ids = pd.concat([unigrams_ids, bigrams_ids])\n",
    "aspects_mapping = aspects_df.merge(aspects_ids, on='aspect')\n",
    "aspects_mapping.drop(labels='aspect', axis=1, inplace=True)\n",
    "aspects_mapping['property_type'] = 'review'\n",
    "aspects_mapping.reindex(columns=['item_id', 'property_type', 'id']).to_csv(os.path.join(all_items_path, 'aspects_mapping.txt'), sep='|', index=False, header=False)\n",
    "aspects_ids.to_csv(os.path.join(all_items_path, 'aspects_list.txt'), sep='|', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taglio aspetti poco connessi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_drop(row, threshold, counter):\n",
    "    return True if counter[row['property_id']] < threshold else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina gli aspetti con un numero di connessioni inferiore a threshold\n",
    "THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_props = pd.read_csv(os.path.join(all_items_path, 'aspects_mapping.txt'), header=None, sep='|', names=['entity_id', 'property_type', 'property_id'])\n",
    "books_per_aspect = Counter(subj_props['property_id'])\n",
    "\n",
    "subj_props['drop'] = subj_props.apply(lambda row: aspect_drop(row, THRESHOLD, books_per_aspect), axis=1)\n",
    "subj_props.drop(subj_props[subj_props['drop'] == True].index, inplace=True)\n",
    "\n",
    "books_per_aspect = Counter(subj_props['property_id'])\n",
    "aspects_per_book = Counter(subj_props['entity_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_props.drop(columns=['drop'], inplace=True)\n",
    "aspects_ids = pd.read_csv(os.path.join(all_items_path, 'aspects_list.txt'), sep='|', names=['property_id', 'property'])\n",
    "aspects_ids = aspects_ids.merge(subj_props, on='property_id', how='inner')\n",
    "aspects_ids.drop(columns=['entity_id', 'property_type'], inplace=True)\n",
    "aspects_ids.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_props.to_csv(os.path.join(output_path, 'aspects_mapping.txt'), sep='|', index=False, header=False)\n",
    "aspects_ids.to_csv(os.path.join(output_path, 'aspects_list.txt'), sep='|', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiche aspetti estratti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_per_aspect = Counter(subj_props['property_id'])\n",
    "aspects_per_book = Counter(subj_props['entity_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is the number of aspects, statistics are books per aspect:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1455.000000\n",
       "mean       24.155326\n",
       "std        29.537085\n",
       "min        10.000000\n",
       "25%        12.000000\n",
       "50%        16.000000\n",
       "75%        25.500000\n",
       "max       618.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('count is the number of aspects, statistics are books per aspect:')\n",
    "pd.Series(books_per_aspect).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is the number of books, statistics are aspects per book:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1485.000000\n",
       "mean       23.667340\n",
       "std        10.955917\n",
       "min         1.000000\n",
       "25%        16.000000\n",
       "50%        23.000000\n",
       "75%        31.000000\n",
       "max        62.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('count is the number of books, statistics are aspects per book:')\n",
    "pd.Series(aspects_per_book).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione file Entity Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata + Aspetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_path = 'entity_recognizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_fuzzy = pd.read_csv(os.path.join(er_path, 'entitiesfuzzy.train'), header=None, names=['property_id', 'property', 'threshold', 'item'])\n",
    "o_regex = pd.read_csv(os.path.join(er_path, 'entitiesregex.train'), sep='\\t', header=None, names=['property', 'item'])\n",
    "o_gazette = pd.read_csv(os.path.join(er_path, 'gazette.txt'), sep='\\t', header=None, names=['item', 'property'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = aspects_ids.copy()\n",
    "fuzzy['threshold'] = 80\n",
    "fuzzy['item'] = 'item'\n",
    "fuzzy = pd.concat([o_fuzzy, fuzzy])\n",
    "fuzzy.to_csv(os.path.join(output_path, 'entitiesfuzzy.train'), sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = aspects_ids.copy()\n",
    "regex.drop('property_id', axis=1, inplace=True)\n",
    "regex['item'] = 'item'\n",
    "regex = pd.concat([o_regex, regex])\n",
    "regex.to_csv(os.path.join(output_path, 'entitiesregex.train'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazette = aspects_ids.copy()\n",
    "gazette.drop('property_id', axis=1, inplace=True)\n",
    "gazette['item'] = 'item'\n",
    "gazette = gazette.reindex(columns=['item', 'property'])\n",
    "gazette = pd.concat([o_gazette, gazette])\n",
    "gazette.to_csv(os.path.join(output_path, 'gazette.txt'), sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solo aspetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fuzzy = pd.read_csv(os.path.join(er_path, 'entitiesfuzzy_aspects_only.train'), header=None, names=['property_id', 'property', 'threshold', 'item'])\n",
    "a_regex = pd.read_csv(os.path.join(er_path, 'entitiesregex_aspects_only.train'), sep='\\t', header=None, names=['property', 'item'])\n",
    "a_gazette = pd.read_csv(os.path.join(er_path, 'gazette_aspects_only.txt'), sep='\\t', header=None, names=['item', 'property'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy = aspects_ids.copy()\n",
    "fuzzy['threshold'] = 80\n",
    "fuzzy['item'] = 'item'\n",
    "fuzzy = pd.concat([a_fuzzy, fuzzy])\n",
    "fuzzy.to_csv(os.path.join(output_path, 'entitiesfuzzy_aspects_only.train'), sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = aspects_ids.copy()\n",
    "regex.drop('property_id', axis=1, inplace=True)\n",
    "regex['item'] = 'item'\n",
    "regex = pd.concat([a_regex, regex])\n",
    "regex.to_csv(os.path.join(output_path, 'entitiesregex_aspects_only.train'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazette = aspects_ids.copy()\n",
    "gazette.drop('property_id', axis=1, inplace=True)\n",
    "gazette['item'] = 'item'\n",
    "gazette = gazette.reindex(columns=['item', 'property'])\n",
    "gazette = pd.concat([a_gazette, gazette])\n",
    "gazette.to_csv(os.path.join(output_path, 'gazette_aspects_only.txt'), sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione Dataset per Algoritmi Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_path = 'content_based/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dataset = pd.read_csv(os.path.join(cb_path, 'books_info.csv'))\n",
    "aspects_dataset = pd.read_csv(os.path.join(output_path, 'aspects_mapping.txt'), sep='|', names=['ID', 'type', 'asp'])\n",
    "\n",
    "rows_list = []\n",
    "for row in books_dataset.itertuples():\n",
    "    new_row = {}\n",
    "    book_asps = aspects_dataset.drop(aspects_dataset[aspects_dataset['ID'] != str(row.ID)].index)\n",
    "    book_asps = book_asps['asp']\n",
    "    book_asps = book_asps.to_list()\n",
    "    str_book_asps = [str(aspect) for aspect in book_asps]\n",
    "    new_row.update({'ID' : row.ID})\n",
    "    new_row.update({'Title' : row.Title})\n",
    "    new_row.update({'Description' : row.Description})\n",
    "    new_row.update({'Subjects' : row.Subjects})\n",
    "    new_row.update({'Authors' : row.Authors})\n",
    "    new_row.update({'Genres' : row.Genres})\n",
    "    new_row.update({'Aspects' : str_book_asps})\n",
    "    rows_list.append(new_row)\n",
    "\n",
    "df = pd.DataFrame(rows_list)\n",
    "df.to_csv(os.path.join(output_path, 'books_info.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bde6861ab66dfd79fbb4b2394a10a0139a08c0887d3b4563f4c9566104718e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('converse': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
